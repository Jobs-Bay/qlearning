{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "963a72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import datetime\n",
    "from gridworld_env import CliffWalkingWapper\n",
    "from agent import QLearning\n",
    "from plot import plot_rewards,plot_rewards_cn\n",
    "from utils import save_results,make_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b536d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6720/311940906.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcurr_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y%m%d-%H%M%S\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#获取当前时间\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcurr_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#当前路径\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mQlearningConfig\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m'''训练相关参数'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "curr_time=datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") #获取当前时间\n",
    "import os\n",
    "curr_path=os.path.dirname(os.path.abspath(__file__)) #当前路径\n",
    "class QlearningConfig:\n",
    "    '''训练相关参数'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.algo=\"Q-learning\" #算法名称\n",
    "        self.env=\"CliffWalking-v0\" #环境名称\n",
    "        self.result_path=curr_path+\"/outputs/\"+self.env+\"/\"+curr_time+\"/results/\" #保存结果的路径\n",
    "        self.model_path=curr_path+\"/outputs/\"+self.env+\"/\"+curr_time+\"/models/\" #保存模型的路径\n",
    "        self.train_eps=400 #训练的回合数\n",
    "        self.eval_eps=30 #测试的回合数\n",
    "        self.gamma=0.9 # reward的衰减率\n",
    "        self.epsilon_start=0.95 #e-greedy策略中初始epsilon\n",
    "        self.epsilon_end=0.01 #e-greedy策略中的终止epsilon\n",
    "        self.epsilon_decay=300 #e-greedy策略中epsilon的衰减率\n",
    "        self.lr=0.1 #学习率\n",
    "        self.device=torch.device(\"cuda\")\n",
    "    \n",
    "def env_agent_config(cfg,seed=1):\n",
    "    env=gym.make(cfg.env)\n",
    "    env=CliffWalkingWapper(env)\n",
    "    env.seed(seed) #设置随机种子\n",
    "    n_states=env.observation_space.n #状态维度\n",
    "    n_actions=env.action_space.n #动作维度\n",
    "    agent=QLearning(n_states,n_actions,cfg)\n",
    "    return env,agent\n",
    "\n",
    "\n",
    "def train(cfg,env,agent):\n",
    "    print(\"开始训练！\")\n",
    "    print(f\"环境：{cfg.env}，算法：{cfg.algo}，设备：{cfg.device}\")\n",
    "    rewards=[] #记录奖励\n",
    "    ma_rewards=[] #记录滑动平均奖励\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        ep_reward=0 #记录每个回合的奖励\n",
    "        state=env.reset() #重置环境，即开始新的回合\n",
    "        while True:\n",
    "            action=agent.choose_action(state) #根据算法选择一个动作\n",
    "            next_state,reward,done,_=env.step(action) #与环境进行一次动作交互\n",
    "            print(reward)\n",
    "            agent.update(state,action,reward,next_state,done) #Q学习算法更新\n",
    "            state=next_state #更新状态\n",
    "            ep_reward+=reward\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        if ma_rewards:\n",
    "            ma_rewards.append(ma_rewards[-1]*0.9+ep_reward*0.1)\n",
    "        \n",
    "        else:\n",
    "            ma_rewards.append(ep_reward)\n",
    "        \n",
    "        print(\"回合数：{}/{}，奖励{:.1f}\".format(i_ep+1,cfg.train_eps,ep_reward))\n",
    "    print(\"完成训练!\")\n",
    "    return rewards,ma_rewards\n",
    "\n",
    "def eval(cfg,env,agent):\n",
    "    print(\"开始测试！\")\n",
    "    print(f\"环境：{cfg.env}，算法：{cfg.algo}，设备：{cfg.device}\")\n",
    "    for item in agent.Q_table.items():\n",
    "        print(item)\n",
    "    \n",
    "    rewards=[]\n",
    "    ma_rewards=[]\n",
    "    for i_ep in range(cfg.eval_eps):\n",
    "        ep_reward=0\n",
    "        state=env.reset()\n",
    "        while True:\n",
    "            action=agent.predict(state) #根据算法选择一个动作\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            state=next_state #更新状态\n",
    "            ep_reward+=reward\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "        if ma_rewards:\n",
    "            ma_rewards.append(ma_rewards[-1]*0.9+ep_reward*0.1)\n",
    "        else:\n",
    "            ma_rewards.append(ep_reward)\n",
    "        \n",
    "        print(f\"回合数：{i_ep+1}/{cfg.eval_eps},奖励：{ep_reward:.1f}\")\n",
    "    print(\"测试完毕！\")\n",
    "    return rewards,ma_rewards\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg=QlearningConfig()\n",
    "env,agent=env_agent_config(cfg,seed=0)\n",
    "make_dir(cfg.result_path,cfg.model_path)\n",
    "agent.save(path=cfg.model_path)\n",
    "for item in agent.Q_table.items():\n",
    "    print(item)\n",
    "save_results(rewards,ma_rewards,tag=\"train\",path=cfg.result_path)\n",
    "plot_rewards_cn(rewards,ma_rewards,tag=\"eval\",env=cfg.env,algo=cfg.algo,path=cfg.result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e1c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
