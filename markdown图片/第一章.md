**Q:状态和观测之间的关系？**

A:**状态**s是对世界的完整描述，不会隐藏世界的信息。观测o是对状态的部分描述，但存在遗漏信息的风险。在深度强化学习中，大多数是用一个实值的向量、矩阵或者更高阶的张量来表示状态和观测。举例，可以用RGB像素值的矩阵来表示一个个视觉的观测，可以用机器人关节的角度和速度来表示一个机器人的状态。

环境以$S_t^e=f^e(H_t)$来更新状态，在智能体的内部也有一个函数$S_t^a=f^a(H_t)$来更新状态。智能体的状态与环境的状态等价的时候，称环境为full observability, 全部可观测。该情况下，强化学习通常被建模成为一个马尔科夫决策过程(MDP)的问题。其中，$O_t=S_t^e=S_t^a$.

其中也会有特例，智能体得到的观测并不能包含环境运作的所有状态，因为在这个强化学习的设定里面，环境的状态才是真正的所有状态。当智能体只能看到部分的观测，我们就称之为部分可观测的(partially observed)。强化学习通常建模成一个部分可观测马尔科夫决策过程(POMDP)的问题。

部分可观测马尔科夫决策过程是一个马尔科夫决策过程的泛化。部分可观测马尔科夫决策过程依然具有马尔科夫性质，但是假设智能体无法感知环境的状态$s$,只能知道部分观测值$o$。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。部分可观测马尔科夫决策过程可以用一个7元组描述：(S,A,T,R,$\Omega$,O,$\gamma$)，其中$S$表示状态空间，为隐变量，A为动作空间，T($s^`|s,a$)为状态转移概率，R为奖励函数，$\Omega(o|s,a)$为观测概率，$O$为观测空间，$\gamma$为折扣系数。

#### 动作空间

不同的环境会有不同种类的动作。在给定的环境中，有效动作的集合经常被称之为**动作空间**。例如：走迷宫机器人如果只有东南西北这4种移动方式，则其为离散动作空间；如果机器人向$360^o$中的任意角度都可以移动，则称之为连续动作空间。

#### 强化学习智能体的组成成分和类型

对于一个强化学习智能体，其可以有一个或者多个如下的组成成分：

- **策略函数(policy function)。** 智能体会用这个函数来选取下一步的动作。
- **价值函数(value function)。**使用价值函数对当前状态进行评估，其就是说进入该状态，可以对后面的收益带来多大的影响。当价值函数越大，说明进入这个状态越有利。
- **模型(model)。**模型表示了智能体对这个环境的状态进行了理解，其决定了这个状态是如何运行的。

#### 策略

深入了解这三个组成部分的一些细节。

**策略(policy)**是智能体的行为模型，其决定了这个智能体的行为，其实际是一个函数，把输入的状态变成行为。这里有两种测略：随机性策略和确定性策略。

**随机性策略(stochastic policy)**就是$\pi(a|s)=P[A_t=a|S_t=s]$。输入一个状态s,输出的是一个概率。这个概率包含了你所有行为的这么一个概率，然后可以进一步对这个概率分布进行采样，得到真实的你采取的行为。比如说这个概率可能有70%的概率向左，30%的概率向右。那么就可以通过一个采样得到一个动作。

**确定性策略(deterministic policy)**就是说你这里有可能只是采取它的极大化，采取最有可能的动作，即$a^*=arg $ $\underset {a}{max}$ $\pi(a|s)$ 。你现在这个概率就是事先决定好的。

通常情况下，强化学习一般使用随机性策略。随机性策略有很多优点。在学习时可以通过引入一定随机性来更好地探索环境；随机性策略的动作具有多样性，这一点在多个智能体博弈时也非常重要。采用确定性策略的智能体总是对同样的环境做出相同的动作，会导致它的策略很容易被对手预测。

#### 价值函数

价值函数是未来奖励的一个预测，用来评估状态的好坏。价值函数里面有一个**折扣因子(discount factor)**，我们希望尽可能在短时间内得到尽可能多的奖励。其实很简单，考虑进去利率因素，自然就成立了；如果我今天给你100元和一个月后给你100元，你一定希望我今天给你100元。是故，将折扣因子放入价值函数的定义里面是合理并且更符合现实，价值函数的定义是一个期望值，如式$(1.3)$所示。

​                                      $\large v_{\pi}(s)\dot{=}\mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s]$,  for all  $\large s\in S$                         $(1.3)$

期望$\mathbb{E}_{\pi}$有个小角标是$\pi$函数，$\pi$函数就是说在我们已知某一个策略函数的时候，到底可以得到多少的奖励。

还有一种价值函数：Q函数。Q函数里面包含两个变量：状态和动作，其定义如式(1.4)所示。

 $\large q_{\pi}(s,a)\dot{=}{\mathbb{E}_{\pi}[G_t|S_t=s,A_t=a]}=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s,A_t=a]$,  for all  $\large s\in S$                         $(1.4)$            

所以你未来可以获得多少的奖励，它这个期望取决于你当前的状态和当前的行为。Q函数是强化学习算法里面要学习的一个函数。因为我们得到Q函数后，进入某一种状态，其最优的行为就可以通过这个Q函数得到。

#### 模型

第三个组成部分是模型，模型决定了下一个状态会是什么样的，就是说下一步的状态取决于你当前的状态以及你当前采取的行动。它由概率和奖励函数两个部分组成，概率是指这个转移状态之间是怎么移动的，如式(1.5)所示。

​                                                $$\large \mathcal{P}_{ss^`}^a=\mathbb{P}[S_{t+1}=s^{`}|S_t=s,A_t=a]$$                                                                                 (1.5)

奖励函数：当你在当前状态采取了某一个行为，可以得到多大的奖励，如式(1.6)所示。

​                                                $$\large \mathcal{R}_{s}^a=\mathbb{E}[R_{t+1}|S_t=s,A_t=a]$$                                                                                             (1.6)

当我们有了这三个组成部分过后，就形成了一个**马尔科夫决策过程(Markv decision process)**。

#### 强化学习智能体的类型

##### 基于价值的智能体与基于策略的智能体

根据智能体学习的东西不同，进行归类。

- **基于价值的智能体(value-based agent)**。这一类智能体显式地学习的是价值函数，隐式地学习了它的策略。策略是从我们学到的价值函数里面推算出来的。
- **基于策略的智能体(policy-based agent)**。这一类智能体直接去学习策略，就是说你直接给它一个状态，其就会输出这个动作的概率。在基于策略的智能体里面并没有去学习它的价值函数。
- **演员-评论员智能体(actor-critic agent)**。这一类智能体将策略函数和价值函数都学习了，然后通过二者交互得到一个最佳的行为。

**Q:基于策略和基于价值的强化学习方法有什么区别？**

A:对于一个状态转移概率已知的马尔可夫决策过程，我们可以使用动态规划算法来求解；从决策方式来看，强化学习又可以划分为基于策略的方法和基于价值的方法。决策方式是智能体在给定状态下从动作集合中选择一个动作的依据，它是静态的，不随状态变化而变化。

在基于策略的强化学习方法中,智能体会制定一套动作策略（确定在给定状态下需要采取何种动作）,并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。而在基于价值的强化学习方法中，智能体不需要制定显式的策略，它维护一个价值表格或价值函数,并通过这个价值表格或价值函数来选取价值最大的动作。基于**价值迭代的方法只能应用在不连续的、离散的环境**下 （如围棋或某些游戏领域），对于**行为集合规模庞大、动作连续的场景**（如机器人控制领域），其很难学习到较好的结果（此时基于**策略迭代的方法**能够根据设定的策略来选择连续的动作)。

基于**价值的**强化学习算法有 Q 学习（Q-learning）、Sarsa 等，而基于**策略的**强化学习算法有策略梯度算法等。此外，**演员-评论员算法同时使用策略和价值评估来做出决策，其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。**

##### 有模型强化学习智能体与免模型强化学习智能体

通过智能体到底有没有学习这个环境模型来分类。

- **有模型（model-based）强化学习智能体**，它通过学习这个状态的转移来采取动作。
- **免模型（model-free）强化学习智能体**，它没有去直接估计这个状态的转移，也没有得到环境的具体转移变量。它通过学习价值函数和策略函数进行决策。免模型的模型里面没有一个环境转移的模型。

我们可以用马尔可夫决策过程来定义强化学习任务，并表示为四元组 $< S, A, P, R >$，即状态集合、动作集合、状态转移函数和奖励函数。如果这四元组中所有元素均已知，且状态集合和动作集合在有限步数内是有限集，则机器可以对真实环境进行建模，构建一个虚拟世界来模拟真实环境的状态和交互反应。

具体来说，当智能体知道状态转移函数$ P(s_{t+1}|s_{t} , a_{t}) $和奖励函数$ R(s_{t} , a_{t}) $后，它就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，这样智能体就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可。这种学习方法称为有模型学习。有模型强化学习的流程图如图 1.20 所示。

<img src="D:/markdown%E5%9B%BE%E7%89%87/1.35.png" alt="img" style="zoom:50%;" />

<center>图1.20 有模型强化学习流程图 </center>

然而在实际应用中，智能体并不是那么容易就能知晓马尔可夫决策过程中的所有元素的。通常情况下，状态转移函数和奖励函数很难估计，甚至连环境中的状态都可能是未知的，这时就需要采用免模型学习。免模型学习没有对真实环境进行建模，智能体只能在真实环境中通过一定的策略来执行动作，等待奖励和状态迁移，然后根据这些反馈信息来更新行为策略，这样反复迭代直到学习到最优策略。

**Q: 有模型强化学习和免模型强化学习有什么区别？**

A: 针对是否需要对真实环境建模，强化学习可以分为有模型学习和免模型学习。有模型学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。

总的来说，有模型学习相比于免模型学习仅仅多出一个步骤，即对真实环境进行建模。因此，一些有模型的强化学习方法，也可以在免模型的强化学习方法中使用。在实际应用中，如果不清楚该用有模型强化学习还是免模型强化学习，可以先思考一下，在智能体执行动作前，是否能对下一步的状态和奖励进行预测，如果可以，就能够对环境进行建模，从而采用有模型学习。

免模型学习通常属于数据驱动型方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略。例如，在雅达利平台上的 Space Invader 游戏中，免模型的深度强化学习需要大约两亿帧游戏画面才能学到比较理想的效果。相比之下，有模型学习可以在一定程度上缓解训练数据匮乏的问题，因为智能体可以在虚拟世界中行训练。免模型学习的泛化性要优于有模型学习，原因是有模型学习算需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型学习算法的泛化性。有模型的强化学习方法可以对环境建模，使得该类方法具有独特魅力，即“想象能力”。在免模型学习中，智能体只能一步一步地采取策略，等待真实环境的反馈；而有模型学习可以在虚拟世界中预测出所有将要发生的事，并采取对自己最有利的策略。

目前，**大部分深度强化学习方法都采用了免模型学习**，这是因为：免模型学习更为简单直观且有丰富的开源资料，像 $AlphaGo $系列都采用免模型学习；在目前的强化学习研究中，大部分情况下环境都是静态的、可描述的，智能体的状态是离散的、可观察的（如雅达利游戏平台），这种相对简单确定的问题并不需要评估状态转移函数和奖励函数，直接采用免模型学习，使用大量的样本进行训练就能获得较好的效果。

如图 1.21 所示，我们可以把几类模型放到同一个饼图里面。饼图有三个组成部分：价值函数、策略和模型。按一个智能体具不具有三者中的两者或者一者可以把它分成很多类。

<img src="https://datawhalechina.github.io/easy-rl/chapter1/img/1.36.png" alt="img" style="zoom:10%;" />

<center>图1.20 强化学习智能体的类型</center>

#### 学习与规划

Learning 和 Planning 是序列决策的两个基本问题。在强化学习中，环境初始时是未知的，智能体不知道环境如何工作，智能体通过不断地与环境交互，逐渐改进策略。

在规划中，环境是已知的，我们被告知了整个环境的运作规则的详细信息。智能体能够计算出一个完美的模型，并且在不需要与环境进行任何交互的时候进行计算。智能体不需要实时地与环境交互就能知道未来环境，只需要知道当前的状态，就能够开始思考，来寻找最优解。

**一个常用的强化学习问题解决思路是**， 先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划。

#### 探索与利用

在强化学习里面，探索和利用是两个很核心的问题。**探索**是说我们怎么去探索这个环境，通过尝试不同的行为来得到一个最佳的策略，得到最大奖励的策略。利用是说我们不去尝试新的东西，就采取已知的可以得到很大奖励的行为。因为在刚开始的时候强化学习智能体不知道它采取了某个行为会发生什么，所以它只能通过试错去探索。所以探索就是在试错来理解采取的这个行为到底可不可以得到好的奖励。**利用**是说我们直接采取已知的可以得到很好奖励的行为。所以这里就面临一个权衡，怎么通过牺牲一些短期的奖励来获得行为的理解，从而学习到更好的策略。

下面举一些探索和利用的例子。

- 以选择餐馆为例，
  - 利用：我们直接去你最喜欢的餐馆，因为你去过这个餐馆很多次了，所以你知道这里面的菜都非常可口。
  - 探索：你把手机拿出来，你直接搜索一个新的餐馆，然后去尝试它到底好不好吃。你有可能对这个新的餐馆非常不满意，钱就浪费了。
- 以做广告为例，
  - 利用：我们直接采取最优的这个广告策略。
  - 探索：我们换一种广告策略，看看这个新的广告策略到底可不可以得到奖励。
- 以挖油为例，
  - 利用：我们直接在已知的地方挖油，我们就可以确保挖到油。
  - 探索：我们在一个新的地方挖油，就有很大的概率，你可能不能发现任何油，但也可能有比较小的概率可以发现一个非常大的油田。
- 以玩游戏为例，
  - 利用：你总是采取某一种策略。比如说，你可能打街霸，你采取的策略可能是蹲在角落，然后一直触脚。这个策略很可能可以奏效，但可能遇到特定的对手就失效。
  - 探索：你可能尝试一些新的招式，有可能你会发出大招来，这样就可能一招毙命。

与监督学习不同，强化学习任务的最终奖赏是在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：最大化单步奖赏，即仅考虑一步操作。需注意的是，即便在这样的简化情形下，强化学习仍与监督学习有显著不同，因为机器需通过尝试来发现各个动作产生的结果，而没有训练数据告诉机器应当做哪个动作。

想要最大化单步奖赏需考虑两个方面：一是需知道每个动作带来的奖赏，二是要执行奖赏最大的动作。若每个动作对应的奖赏是一个确定值，那么尝试遍所有的动作便能找出奖赏最大的动作。然而，更一般的情形是，一个动作的奖赏值是来自于一个概率分布，仅通过一次尝试并不能确切地获得平均奖赏值。

$K-armed$   $bandit$是一种单步强化学习任务对应的一个理论模型($multi-armed$ $bandit$)。如图 1.24 所示，K-摇臂赌博机有 K 个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。若仅为获知每个摇臂的期望奖赏，则可采用**仅探索（exploration-only）法**：将所有的尝试机会平均分配给每个摇臂（即轮流按下每个摇臂），最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。若仅为执行奖赏最大的动作，则可采用仅**利用（exploitation-only）法**：按下目前最优的（即到目前为止平均奖赏最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。

显然，仅探索法能很好地估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会；仅利用法则相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖赏最大化。

事实上，探索（即估计摇臂的优劣）和利用（即选择当前最优摇臂) 这两者是矛盾的，因为尝试次数（即总投币数）有限，加强了一方则会自然削弱另一方，这就是强化学习所面临的**探索-利用窘境（exploration-exploitation dilemma）**。显然，想要累积奖赏最大，则必须在探索与利用之间达成较好的折中。

<img src="D:/markdown%E5%9B%BE%E7%89%87/1.39.png" alt="img" style="zoom:50%;" />

<center>图1.24 K-armed bandit</center>

#### 强化学习实验

强化学习是一个理论跟实践结合的机器学习分支，我们不仅要理解它算法背后的一些数学原理，还要上机实践通过实现算法。在很多实验环境里面去探索这个算法能不能得到预期效果也是一个非常重要的过程。

##### Gym

$OpenAI $的 Gym 是一个环境仿真库，里面包含了很多现有的环境。针对不同的场景，我们可以选择不同的环境。离散控制场景（输出的动作是可数的，比如 $Pong $游戏中输出的向上或向下动作）：一般使用雅达利环境评估；连续控制场景（输出的动作是不可数的，比如机器人走路时不仅有方向， 还要角度，角度就是不可数的，是一个连续的量）：一般使用 $mujoco $环境评估。Gym Retro 是对 Gym 环境的进一步扩展，包含了更多的一些游戏。

我们可以通过 pip 来安装 Gym:

```python
pip install gym
```

在 Python 环境中导入Gym，如果不报错，就可以认为 Gym 安装成功。

```python
$python
>>>import gym
```

强化学习的这个交互就是由 agent 跟环境进行交互。所以算法的 interface 也是用这个来表示。比如说我们现在安装了 OpenAI Gym。

1. 我们就可以直接调入 Taxi-v3 的环境，就建立了这个环境。
2. 初始化这个环境过后，就可以进行交互了。
3. Agent 得到这个观测过后，它就会输出一个 action。
4. 这个动作会被环境拿进去执行这个 step，然后环境就会往前走一步，返回新的 observation、reward 以及一个 flag variable `done` ，`done` 决定这个游戏是不是结束了。

几行代码就实现了强化学习的框架。

```python
import gym 
env = gym.make("Taxi-v3") 
observation = env.reset() 
agent = load_agent() 
for step in range(100):
    action = agent(observation) 
    observation, reward, done, info = env.step(action)
```

在 Gym 里面有很经典的控制类游戏，如图 1.26 所示。比如说$ Acrobot$ 就是把两节铁杖甩了立起来。 $CartPole $是通过控制一个平板，让木棍立起来。$MountainCar$ 是通过前后移动这个车，让它到达这个旗子的位置。

在刚开始测试强化学习的时候，可以选择这些简单环境，因为这些环境可以在一两分钟之内见到效果。

<img src="https://datawhalechina.github.io/easy-rl/chapter1/img/1.46.png" alt="img" style="zoom:10%;" />

<center>图1.26 经典控制问题</center>

这里我们看一下 $CartPole $环境。如图 1.27 所示，对于这个环境，有两个动作，Cart 往左移还是往右移。这里得到了观测：这个车当前的位置，Cart 当前往左往右移的速度，这个杆的角度以及杆的最高点的速度。

如果观测越详细，就可以更好地描述当前这个所有的状态。这里有奖励的定义，如果能多保留一步，你就会得到一个奖励，所以你需要在尽可能多的时间存活来得到更多的奖励。当这个杆的角度大于某一个角度（没能保持平衡）或者这个车已经出到外面的时候，游戏就结束了，你就输了。所以这个智能体的目的就是为了控制木棍，让它尽可能地保持平衡以及尽可能保持在这个环境的中央。

<img src="https://datawhalechina.github.io/easy-rl/chapter1/img/1.47.png" style="zoom:10%;" />

<center>图1.27 CartPole-vo的例子</center>

```python
import gym  # 导入 Gym 的 Python 接口环境包
env = gym.make('CartPole-v0')  # 构建实验环境
env.reset()  # 重置一个 episode
for _ in range(1000):
    env.render()  # 显示图形界面
    action = env.action_space.sample() # 从动作空间中随机选取一个动作
    env.step(action) # 用于提交动作，括号内是具体的动作
env.close() # 关闭环境
```

注意：如果绘制了实验的图形界面窗口，那么关闭该窗口的最佳方式是调用`env.close()`。试图直接关闭图形界面窗口可能会导致内存不能释放，甚至会导致死机。

当你执行这段代码时，机器人会完全无视那根本该立起来的杆子，驾驶着小车朝某个方向一通跑，直到不见踪影，这是因为我们还没开始训练机器人。

Gym 中的小游戏，大部分都可以用一个普通的实数或者向量来充当动作。打印 `env.action_space.sample()` 的返回值，能看到输出为 1 或者 0。

`env.action_space.sample()`的含义是，在该游戏的所有动作空间里随机选择一个作为输出。在这个例子中，意思就是，动作只有两个：0 和 1，一左一右。

`env.step()`这个方法的作用不止于此，它还有四个返回值，分别是`observation`、`reward`、`done`、`info`。

- `observation(object)`是状态信息，是在游戏中观测到的屏幕像素值或者盘面状态描述信息。
- `reward(float)`是奖励值，即 action 提交以后能够获得的奖励值。这个奖励值因游戏的不同而不同，但总体原则是，对完成游戏有帮助的动作会获得比较高的奖励值。
- `done(boolean)`表示游戏是否已经完成。如果完成了，就需要重置游戏并开始一个新的 episode。
- `info(dict)`是一些比较原始的用于诊断和调试的信息，或许对训练有帮助。不过，$OpenAI $团队在评价你提交的机器人时，是不允许使用这些信息的。

在每个训练中都要使用的返回值有 observation、reward、done。但 observation 的结构会由于游戏的不同而发生变化。以$ CartPole-v0 $小游戏为例，我们修改下代码：

```python
import gym  
env = gym.make('CartPole-v0')  
env.reset()  
for _ in range(1000):
    env.render()  
    action = env.action_space.sample() 
    observation, reward, done, info = env.step(action)
    print(observation)
env.close()
```

输出：

```python
[ 0.01653398  0.19114579  0.02013859 -0.28050058]
[ 0.0203569  -0.00425755  0.01452858  0.01846535]
[ 0.02027175 -0.19958481  0.01489789  0.31569658]
......
```

从输出可以看出这是一个四维的 Observation。在其他游戏中会有维度很多的情况。

`env.step()`完成了一个完整的 $S \to A \to R \to S'$ 过程。我们只要不断观测这样的过程，并让机器在其中用相应的算法完成训练，就能得到一个高质量的强化学习模型。

想要查看当前 Gym 库已经注册了哪些环境，可以使用以下代码：

```python
from gym import envs
env_specs = envs.registry.all()
envs_ids = [env_spec.id for env_spec in env_specs]
print(envs_ids)
```

每个环境都定义了自己的观测空间和动作空间。环境 env 的观测空间用`env.observation_space`表示，动作空间用 `env.action_space `表示。观测空间和动作空间既可以是离散空间（即取值是有限个离散的值），也可以是连续空间（即取值是连续的）。在 Gym 库中，离散空间一般用`gym.spaces.Discrete`类表示，连续空间用`gym.spaces.Box`类表示。

例如，环境`'MountainCar-v0'`的观测空间是`Box(2,)`，表示观测可以用 2 个 float 值表示；环境`'MountainCar-v0'`的动作空间是`Dicrete(3)`，表示动作取值自`{0,1,2}`。对于离散空间，`gym.spaces.Discrete`类实例的成员 n 表示有几个可能的取值；对于连续空间，`Box`类实例的成员 low 和 high 表示每个浮点数的取值范围。

##### MountainCart-vo 例子

接下来，我们通过一个例子来学习如何与 Gym 库进行交互。我们选取 `小车上山(MountainCar-v0)`作为例子。

首先我们来看看这个任务的观测空间和动作空间：

```python
import gym
env = gym.make('MountainCar-v0')
print('观测空间 = {}'.format(env.observation_space))
print('动作空间 = {}'.format(env.action_space))
print('观测范围 = {} ~ {}'.format(env.observation_space.low,
        env.observation_space.high))
print('动作数 = {}'.format(env.action_space.n))
```

输出：

```python
观测空间 = Box(2,)
动作空间 = Discrete(3)
观测范围 = [-1.2  -0.07] ~ [0.6  0.07]
动作数 = 3
```

由输出可知，观测空间是形状为 (2,) 的浮点型 np.array，动作空间是取 {0,1,2} 的 int 型数值。

接下来考虑智能体。智能体往往是我们自己实现的。我们可以实现一个智能体类：`BespokeAgent类`，代码如下所示：

```python
class BespokeAgent:
    def __init__(self, env):
        pass
    
    def decide(self, observation): # 决策
        position, velocity = observation
        lb = min(-0.09 * (position + 0.25) ** 2 + 0.03,
                0.3 * (position + 0.9) ** 4 - 0.008)
        ub = -0.07 * (position + 0.38) ** 2 + 0.07
        if lb < velocity < ub:
            action = 2
        else:
            action = 0
        return action # 返回动作

    def learn(self, *args): # 学习
        pass
    
agent = BespokeAgent(env)
```

智能体的 `decide()` 方法实现了决策功能，而 `learn()` 方法实现了学习功能。`BespokeAgent`类是一个比较简单的类，它只能根据给定的数学表达式进行决策，不能有效学习。所以它并不是一个真正意义上的强化学习智能体类。但是，用于演示智能体和环境的交互已经足够了。

接下来我们试图让智能体与环境交互，代码如下所示：

```python
def play_montecarlo(env, agent, render=False, train=False):
    episode_reward = 0. # 记录回合总奖励，初始化为0
    observation = env.reset() # 重置游戏环境，开始新回合
    while True: # 不断循环，直到回合结束
        if render: # 判断是否显示
            env.render() # 显示图形界面，图形界面可以用 env.close() 语句关闭
        action = agent.decide(observation)
        next_observation, reward, done, _ = env.step(action) # 执行动作
        episode_reward += reward # 收集回合奖励
        if train: # 判断是否训练智能体
            agent.learn(observation, action, reward, done) # 学习
        if done: # 回合结束，跳出循环
            break
        observation = next_observation
    return episode_reward # 返回回合总奖励
```

上面代码中的 `play_montecarlo` 函数可以让智能体和环境交互一个回合。这个函数有 4 个参数：

- `env` 是环境类
- `agent` 是智能体类
- `render`是 bool 类型变量，指示在运行过程中是否要图形化显示。如果函数参数 render为 True，那么在交互过程中会调用 `env.render()` 以显示图形化界面，而这个界面可以通过调用 `env.close()` 关闭。
- `train`是 bool 类型的变量，指示在运行过程中是否训练智能体。在训练过程中应当设置为 True，以调用 `agent.learn()` 函数；在测试过程中应当设置为 False，使得智能体不变。

这个函数有一个返回值 `episode_reward`，是 float 类型的数值，表示智能体与环境交互一个回合的回合总奖励。

接下来，我们使用下列代码让智能体和环境交互一个回合，并在交互过程中图形化显示，可用 `env.close()` 语句关闭图形化界面。

```python
env.seed(0) # 设置随机数种子,只是为了让结果可以精确复现,一般情况下可删去
episode_reward = play_montecarlo(env, agent, render=True)
print('回合奖励 = {}'.format(episode_reward))
env.close() # 此语句可关闭图形界面
```

输出：

```python
回合奖励 = -105.0
```

为了系统评估智能体的性能，下列代码求出了连续交互 100 回合的平均回合奖励。

```python
episode_rewards = [play_montecarlo(env, agent) for _ in range(100)]
print('平均回合奖励 = {}'.format(np.mean(episode_rewards)))
```

输出：

```python
平均回合奖励 = -102.61
```

小车上山环境有一个参考的回合奖励值 -110，如果当连续 100 个回合的平均回合奖励大于 -110，则认为这个任务被解决了。`BespokeAgent` 类对应的策略的平均回合奖励大概就在 -110 左右。

测试 agent 在 Gym 库中某个任务的性能时，学术界一般最关心 100 个回合的平均回合奖励。至于为什么是 100 个回合而不是其他回合数（比如 128 个回合），完全是习惯使然，没有什么特别的原因。对于有些环境，还会指定一个参考的回合奖励值，当连续 100 个回合的奖励大于指定的值时，就认为这个任务被解决了。但是，并不是所有的任务都指定了这样的值。对于没有指定值的任务，就无所谓任务被解决了或者没有被解决。

总结一下 Gym 的用法：使用 `env=gym.make(环境名)` 取出环境，使用 `env.reset()`初始化环境，使用`env.step(动作)`执行一步环境，使用 `env.render()`显示环境，使用 `env.close()` 关闭环境。

最后提一下，Gym 有对应的[官方文档](https://gym.openai.com/docs/)，大家可以阅读文档来学习 Gym。

